The Syntacs Translation Toolkit (`STT') is a lexer/parser generator in
the lex/yacc family.

   This document does not explain the theory of finite automata,
shift-reduce parsing, or any other of the topics one typically
encounters in the first section of any compiler book.  Although it is
not strictly essential to be familiar with these concepts, it's much
easier going if you do.

1 Overview
**********

1.1 What
========

The Syntacs Translation Toolkit (`STT') is a lexer/parser generator in
the lex/yacc family.  Given an input grammar, the `STT' will generate a
"translator", a machine that partially implements the lexing and parsing
tasks in Java.  The grammar author is responsible for implementing an
"Interpreter" that is used to guide the instantiation of terminals and
nonterminals throughout the translation process.  The Interpreter takes
the place of "semantic actions" which are traditionally interleaved into
a grammar.

1.2 How
=======

Lexical analysis is done using traditional finite automata.  The notion
of stack-managed lexer states ("lexical context") is introduced.
Syntactic analysis is done using shift-reduce parsing using either SLR1,
LR1, or LALR1 parse tables.

1.3 Why
=======

No reason; just because.  Well, that's not exactly true - I was writing
a new language of my own and of course the first thing one does when
writing a new language is to write one's own lexer and parser generator
- yes, to pretty much reinvent the wheel.  The thing is is that one
needs to understand the wheel, and the best way to do that is to
reinvent it.  So it was/is an educational experience.

2 Definitions
*************

It is useful to define some terms that are used throughout the STT
related to the theory of lexical and syntactic analysis.  Note that the
concepts defined here are reworked to my own interpretation and specific
to the STT, they are not intended to be generally applicable.

2.1 Lexical Definitions
=======================

   * A "regular expression" is a language that defines what set of input
     character sequences "reduce" to a token.

   * A "token" is an input character sequence that is a particular
     instance of a regular expression.

   * A "regular definition" is a regular expression that has a "symbol"
     (a name).

   * A "regular grammar" is a set of regular definitions.

   * A "lexical context" is a regular grammar that has a "symbol" (a
     name).

   * A "context transition function" is a mapping from the tuple
     "(context symbol, regular definition symbol)" to "context action".

   * A "context action" is a tuple "(instruction, value)" that drives
     the management of the lexer context stack.

   * A "lexical grammar" is a set of lexical contexts, a context
     transition function, and a "start context" (a distinguished context
     in the lexical grammar).

   * A "deterministic finite automaton" (DFA) is an regular expression
     recognition machine that has a "transition function" and an "output
     function".  The output function maps state numbers to regular
     definition names.  A DFA is equivalent to a single regular grammar.

   * A lexer is machine that transforms an input sequence of characters
     to an output sequence of tokens.  It uses a set of DFAs (one for
     each lexical context), the context transition function, and a
     context stack.

2.2 Syntactic Definitions
=========================

   * A "context-free grammar" is a set of terminals, a set of
     nonterminals, and an "accepting nonterminal" (a distinguished
     nonterminal).

   * A "grammar symbol" is the union of the set of terminal symbols and
     the set of nonterminal symbols.

   * A "terminal" is a symbol.  The set of symbols that constitute the
     terminals is typically drawn from the union of set of regular
     definition symbols over all the lexical contexts of the lexical
     grammar.  Note that not all regular definition symbols need be
     terminal symbols, and not all terminal symbols need be defined as a
     regular definition (see fig 1).

   * a "context-free expression" is a language that defines which
     grammar symbol sequences "reduce" to a nonterminal.

   * A "production" is a single partition of a context-free expression,
     it is one alternative of a nonterminal.

   * A "nonterminal" is context-free expression that has a "symbol".

   * A "deterministic pushdown automaton" (DPA) is an context-free
     expression recognition engine that has a terminal transition
     function and a nonterminal transition function.  A DPA is
     equivalent to a single context-free grammar.

   * A "parser" is a machine that transforms an input sequence of
     terminals to a tree of grammar symbols (the syntax tree).  It uses
     a DPA, a state stack, and a symbol stack.

             +---------------+
             |   Terminals   |
             |               |
        +----|----------+    |
        |    |          |    |
        |    +---------------+
        |               |
        | Regular Dfn's |
        +---------------+
   Fig. 1: The set of terminal symbols and the set of regular definition
symbols are not necessarily equal.

2.3 Translative Definitions
===========================

   * "Translation" refers to the combined process of lexical and
     syntactic analysis.

   * A "translator grammar" is a lexical grammar and a context-free
     grammar.

   * A "translator" is a machine that transforms an input character
     sequence to an output object.  The output object may be the syntax
     tree itself or some result that is generated through
     "interpretation" of the (abstract) syntax tree.

3 Processing Model
******************

This section describes the sequence of events that occur during a
translation run and the components that are involved.  Below is a
graphical summary of how these components interact, refer to it as
necessary.


                   SUMMARY OF PROCESSING MODEL

                             (1) user invokes `trnsltr.translate(in)'.
            +-----------+    (2) trnsltr initializes Input with `in' argument.
            |           |    (3) trnsltr initializes Auditor (the error repository).
            | The User  |    (4) trnsltr invokes `lexer.start()', begins parse loop.
            |           |    (5) lexer returns, parse is complete.
            +-----------+    (6) trnsltr requests result Object from ParserInterpreter.
              ||     /\      (7) ParserInterpreter returns result.
              || (1) || (8)  (8) trnsltr returns result to user, translation is complete.
              \/     ||
            +------------------------------------------+
            | Translator                               |
            +------------------------------------------+
              ||     /\     /\             ||       ||
              || (4) || (5) || (6,7)       || (2)   ||
       >>     \/     ||     ||             \/       ||
       __   +---------------------+       +---+     ||
      '  `  | Lexer               | <---> |I  |     || (3)
      |M |  +---------------------+       |N  |     \/
      |A |    ||            ||  |         |P  |    +---+
      |I |    || (a)        ||  `---------|U  |--> |A  |
      |N |    \/            ||            |T  |    |U  |
      |  |  +=====================+       |   |    |D t|
      |P |  | LexerInterpreter    | <---> |   |    |I h|
      |A |  +=====================+       |   |    |T e|
      |R |    ||            ||  |         |   |    |O  |
      |S |    || (b)        ||  `---------|   |--> |R e|
      |E |    \/            ||            |   |    |  r|
      |  |  +---------------------+       |   |    |  r|
      |L |  | Parser              | <---> |   |    |  o|
      |O |  +---------------------+       |   |    |  r|
      |O |    ||     /\     ||  |         |   |    |   |
      |P |    || (c) || (d) ||  `---------|   |--> |  m|
      |  |    \/     ||     \/            |   |    |  a|
      |  |  +=====================+       |   |    |  n|
      |  |  | ParserInterpreter   | <---> |   |    |  a|
      `__'  +=====================+       +---+    |  g|
                                |                  |  e|
       <<                       `----------------> |  r|
                                                   +---+
         PARSE LOOP SUMMARY
        --------------------

        (a) Lexer uses Input & DFA[]
            recognizes terminal character sequence (match event)
            calls `interpreter.match(int terminal_type, int offset, int length)'

        (b) LexerInterpreter interprets match event
            packages terminal as a Symbol
            calls `parser.notify(Symbol terminal)'

        (c) Parser uses Symbol & DPA
            recognizes nonterminal symbol sequence (reduction event)
            calls `interpreter.reduce(int production_type, Sentence stack)'

        (d) ParserInterpreter interprets reduction
            packages nonterminal as a Symbol
            returns nonterminal to Parser for inclusion on parse stack

        * All components may interact with Input and Auditor as necessary

        * LexerInterpreter and ParserInterpreter (double-lined boxes) are
          generally a single object that implements LRTranslatorInterpreter

3.1 Translation Components
==========================

There are several components that make up a translator.  They are listed
here as interfaces, which is how they are exist in the API, but a
translator that has been generated by the `STT' may or may not retain
these abstractions (for performance reasons).

3.1.1 Input
-----------

Responsible for directly managing the input character sequence; it
tracks position and line status.  When the input has been exhausted, it
will trigger an end-of-file signal.

3.1.2 Lexer
-----------

Responsible for "transforming" the fine-grained sequence of characters
into a larger-grained sequence of tokens.  It "uses" an `Input' object
and a set of finite automatons (`DFA's).  When a token is recognized, it
notifies the `LexerInterpreter' that a token recognition event has
occurred; this event describes what token was matched, where in the
input the token starts, and how long the token is.  Context switching
also occurs within the `Lexer'.

3.1.3 LexerInterpreter
----------------------

Responsible for "listening" for lexer events, "packaging" tokens as
terminal symbols, and "notifying" the `Parser' of these terminals.  It
is therefore the intermediary between the "Lexer" and the "Parser" that
"interprets" lexer events.

   Code within the `LexerInterpreter' is typically a large switch
statement with a case for each token type.  _The user is responsible for
writing this code_.

   One of the advantages of using this type of event model is that
`String' creation is minimized.  No since most tokens are either ignored
(comments and whitespace) or syntactic placeholders (which carry all
their meaning in their name, such as a parenthesis), no `String' or
array copying needs to occur in these instances.  It gives the user
complete control of what is passed to the parser.  For example, if the
`LexerInterpreter' recieves an `Identifier' event, it could do a keyword
symbol table lookup and then pass the correct keyword terminal to the
parser.

   This also gives a chance to instantiate terminals that may form part
of the final syntax tree, minimizing transformations later in the
processing cycle.

3.1.4 Parser
------------

Responsible for transforming a serial stream of tokens into a syntax
tree; it employs an augmented finite automaton (`DPA') as the
recognition engine and maintains a parse stack of the symbols.

   When the parser is notified of a terminal `Symbol', it consults the
DPA to see what to do.  If the action is "reduce", the parser will
delegate nonterminal construction to the `ParserInterpreter'.  This
delegation of `Symbol' construction responsibility to the
`ParserInterpreter' is analogous to the relationship between the `Lexer'
and the `LexerInterpreter'.  When the `ParserInterpreter' returns, it
passes back a nonterminal symbol to be placed on the top of the parse
stack as per normal shift-reduce parsing.

   When the DPA says "accept", the `accept()' method is called on the
`ParserInterpreter'.

3.1.5 ParserInterpreter
-----------------------

Responsible for "listening" for parse events and "packaging" nonterminal
symbols to the parser.  When the parser sees that a reduction is
necessary, it sends the `ParserInterpreter' a number identifying what
production needs to be reduced as well as another object (called a
`Sentence'), which is the exposed top of the parse stack.  The
`ParserInterpreter' then decides what symbols in the `Sentence' (i.e.
stack) to keep, packages them as a [new] nonterminal, and returns that
to the `Parser'.  The `Parser' then places this symbol on the top of the
(now reduced) parse stack.

   Code within the `ParserInterpreter' is typically a large switch
statement with a case for each production type.  _The user is
responsible for writing this code_.

   When the parser discovers a syntactic error, it consults the
`ParserListener' for instruction on how to "recover" from the error.
The ParserListener formulates a list of "corrections" and returns this
to the parser for execution.  When these corrections are "satisfied",
normal parsing resumes.

3.1.6 LRTranslatorInterpreter
-----------------------------

Typically, the `LexerInterpreter' and `ParserInterpreter' are a single
object that implements `LRTranslatorInterpreter', which is formed by the
union of these interfaces.  For examples, consult the
`RegexpInterpreter' and the `SyntacsInterpreter' classes.

   The `LRTranslatorInterpreter' has the additional responsibility of
packaging the final result for the `Translator'.  The translator
instance will retrieve this object through the `getResult()' method.
The interpreter only need return a meaningful result if the input is
accepted.

3.1.7 Translator
----------------

A `Translator' is an object that abstracts the lower-level structural
analysis; it is the thing the user interacts with.  Internally, a
`Translator' may or may not use all of the other components listed
above.  It manages initialization of the parse, runs it, and then
returns the result `Object' to the user.

   If the translation is not error-free, a `TranslationException' is
thrown.  This exception carries out the `Auditor' (the repository for
errors and warnings) to the user.

   In summary, a simplified schematic for the translation is given
below.

     (1) User invokes `translate()'
     (2) Translator runs the internal parse loop
     (3) Translator fetches the result from the Interpreter
     (4) Result is returned to the User

   Fig 2:  Simplified Processing Model

4 Compilation Model
*******************

When a grammar is compiled, each component in the translator is
constructed.

4.1 Construction of LRTranslatorGrammar
=======================================

The `LRTranslatorGrammar' is constructed via parsing of either a file in
the "native syntacs format" or `XML'.  If the parse is error-free,
additional semantic checks are done to make sure that the grammar is
internally consistent.

4.2 Construction of Lexical Analyzer
====================================

Each context in the `LRTranslatorGrammar' is transformed into a
`RegularGrammar' instance, which is further transformed to a `DFA'.  The
`DFA' is then transformed (compressed) and integrated into the `Lexer'.

4.3 Construction of the Syntactic Analyzer
==========================================

The `LRTranslatorGrammar' is used to build a `ContextFreeGrammar' which
is then transformed into a "deterministic pushdown automaton" (`DPA').
The `DPA' is compressed and integrated into the `Parser'.

4.4 Construction of the LRTranslatorInterpreter
===============================================

The `LRTranslatorInterpreter' is implemented by the grammar author and
named in the `compile-interpreter-classname' property such that it may
be reflected by the grammar compiler and integrated into the
`Translator'.  If no such property is given, a default
`LRTranslatorInterpreter' is used.

   Fig 3: Schematic showing how a `Translator' is constructed.

5 Lexical Context
*****************

Background
==========

The concept of "lexer states" was introduced by the flex scanner
generator as a means to subdivide the lexer into multiple layers such
that only a subset of the token definitions can be matched at a
particular time.  This is useful when a syntax has widely variable
"topography"; certain sections of a file are syntatically very different
from one another.  Attempts to cram all the required regular expressions
into a a single "lexer state" (a single DFA) result in collisions within
the grammar. Under these circumstances it becomes advantageous to
partition the token definitions into subsets and switch between them at
the appropriate time.

   *Note*: I would imagine that the concept of lexer states was almost
certainly known well before flex came along, but my knowledge of the
subject is pretty limited so don't quote me on historical accuracy.

Transition Function
===================

In the `STT', this notion of "lexer states" is recast into the term
"lexical context", or more commonly just "context", for short.  Note
however its meaning here is unrelated to the term "context-free
parsing"; that is, "lexical context" does not imply any sort of
"context-ful parsing".  This is because the notion of lexical context
does not affect the theory or algorithms used in the parsing phase
(syntactic analysis).

   It is useful to remember the formula that "one lexical context equals
one DFA".  With this in mind then we can restate what it means to be a
lexical analyzer in the `STT': A lexical analyzer is a machine that
transforms a stream of characters into a stream of tokens; to accomplish
its job it requires a "set of contexts" and a "context transition
function".  The transition function is a mapping from `(context, token)
--> context'.

   Therefore, when a DFA says "hey, we found the beginning of a
comment!", the context transition function is consulted to check if the
context needs to be changed.

Context Stack
=============

`STT' lexers have an additional feature in that they maintain a "context
stack".  I fibbed when I said that the transition function is a mapping
from `(context, token) --> context'; it is actually a mapping `(context,
token) --> context action' where the `context action' is a "context
stack instruction".  A context action is a tuple `(instruction,
register)' and hence the full definition for the transition function is
a tuple to tuple mapping `(context, token) --> (instruction, register)'.

   The instruction is a constant that is one of:

   * *`PEEK'*: This is the no-op instruction; it says to change contexts
     to the one you are currently in, meaning "do nothing".  The
     `register' for a `PEEK' instruction does not hold a meaningful
     value.

   * *`PUSH'*: This instruction switches to the context named in the
     `register' and places it on the top of the stack.

   * *`POP'*: This instruction removes the top element of the context
     stack, throws it away, and then switches into the context at the
     new top of the stack.  This has the effect of going back to the
     context you were in before a `PUSH'. The `register' for a `POP'
     instruction does not hold a meaningful value.

How to Use Multiple Contexts
============================

To take advantage of multiple lexical contexts you need to declare the
names of the contexts to be used in your grammar.  Then, define which
terminals will be included in what context such that the finite automata
can be correctly assembled for each context.  The context definition is
also where PUSH or POP instructions are made.  *Note Grammar Syntax::,
for more information.

6 Error Handling
****************

An important part of any parser generator is error recovery and/or error
repair.  The design that the STT uses is that when an error is
discovered by the parser, it asks to the `ParserInterpreter' how to
recover.  This, in theory, gives the user complete control over error
recovery.  In practice, however, only a few maneuvers (called
"Corrections") are defined by the parser, but these are sufficient for
general use.

6.1 Auditor
===========

The "auditor" is the central listener for errors and warnings,
collectively called "complaints".  The auditor instance is visible to
all translation components.  When a `TranslationException' is thrown,
the auditor instance is carried out on the back of the exception to the
caller such that it can be printed out or otherwise inspected.

6.2 Lexical Errors
==================

If the lexer cannot find an appropriate match given the current input,
it will notify the `LexerInterpreter' through the `error(int offset, int
length)' method with the offset and length of the unrecognizable
sequence.  No context switching occurs during a lexical error.

   It is standard behavior for the lexer interpreter to report this to
the `Auditor' such that the line is printed, highlighting the syntax
error.

   Currently, no lexical error handling is done in the lexer itself.
The overall design, however, allows custom lexical error handling to be
implemented by the author within the `LexerInterpreter'.

6.3 Syntactic Errors
====================

If for a given parser state and input terminal no valid action is known,
the parser will react by calling `interpreter.recover(int type, Sentence
left_context)'.  The return value from the `ParserInterpreter' is a
`Recovery' object (a list of `Correction' actions that the parser will
execute in sequence).

   This design is relatively flexible because it allows for future
implementation of a number of methods for error recovery.  However, only
a few `Correction' types have been implemented thus far!  This is
partically due to the fact that (1) error recovery is for some reason
one of the last things implemented in these types of projects, (2) the
"SYNC and TUMBLE" recovery paradigm works nicely and is extremely
simple.

6.4 Correction Types
====================

6.4.1 ABORT
-----------

When the parser executes an ABORT correction, it bails out by
immediately throwing a `TranslationException'.  This is useful if a
maximum number of errors have been discovered (such as 100).  In this
case the user would be responsible for checking with the auditor how
many errors the translation has encountered.

6.4.2 SYNC
----------

When the parser executes a SYNC correction, it will discard future input
terminals from the `LexerInterpreter' until it sees one that matches
some given type (a semicolon, for example).  The terminal named by the
SYNC instruction is called a "synchronizing symbol".

   Note that the synchronizing symbol is also discarded.  When the SYNC
instruction is satisfied, the parser advances to the next correction in
the recovery list.

6.4.3 TUMBLE
------------

When the parser executes a TUMBLE currection, it iteratively challenges
the current input terminal symbol and the current parse state against
the `DPA.action(state, symbol)' method.

   If the `Action' returned by the DPA is not an ERROR, the TUMBLE
condition is satisfied, the parser executes the given `Action', and the
parser advances to the next correction in the recovery plan.  If the end
of the recovery plan has been reached, normal parsing resumes.

   Conversely, if the `Action' is an ERROR, the parser pops both the
state and symbol stacks, discards their values, and tries another
challenge.  In this manner the parser is "tumbling" down the stack
trying to find a combination that works.

   For grammars that have obvious synchronizing tokens, the SYNC and
TUMBLE recovery combo works well to limit the number of errors to a
reasonable number.  I hope to implement more error correction types in a
future release as it is an interesting problem.  I would also encourage
others to get involved.  Your help is needed!

7 Grammar Syntax
****************

A grammar for a translator generated with the `STT' is an "STT Grammar".
Either the "native .stt format" or `XML' can be used to express the
required structure.

XML Format
==========

Though `XML' can be used as to write a grammar, it is far more verbose
than the native stt format.  Since the abstract structure of an `XML'
grammar instance and an stt grammar instance are interchangeable, no
formal description of the `XML' format is given; consult the `DTD' and
the examples in the distribution.  The use of XML was basically a
bootstrap mechanism.  It is still occasionally required when some part
of the translation machinery is broken due to development, disabling the
native pathway.

   `XML' instances must conform to the `grammar.dtd' document type.

STT Format
==========

A grammar file consists of a set of sections, some of which are
optional. Each section consists of one or more "statements" terminated
by a semicolon.

   Comments and whitespace are discarded.  Comments are typical
unix-style; they start with a pound sign (`#') and end with a newline.

   The sections are:

   * *Grammar Declaration*: defines the grammar name and version
     [required].

   * *Property Definitions*: declares the names and values of various of
     properties [optional].

   * *Terminal Declarations*: declares the names of terminals (tokens)
     [required].

   * *Terminal Definitions*: assocations between the terminal names and
     regular expressions [optional].

   * *Nonterminal Declarations*: declares the names of nonterminals
     [required].

   * *Nonterminal Definitions*: defines the productions used to control
     parse stack reductions [required].

   * *Accept Definition*: defines the nonterminal to be used as the goal
     symbol [required].

   * *Context Declarations*: declares the names of the lexical contexts
     [optional].

   * *Context Definitions*: defines what terminals are to be included in
     what context [optional].

   * *Start Context Definition*: defines what context is the initial
     context [optional].

7.1 Grammar Declaration
=======================

The grammar declaration defines the name of the grammar and the version.
It looks like this:

     # format: this is <NAME> version <VERSION>;
     this is syntacs version 0.1.0;

7.2 Property Declarations
=========================

Properties are key:value pairs that are put into a hashtable and used
throughout grammar processing.  *Note Properties::, for a listing of
these properties. They are enclosed in double-quotes.

     # format: property <NAME> = "<VALUE>";
     property namespace = "com.inxar.syntacs.translator.regexp";

7.3 Terminal Declarations
=========================

Terminals need to be declared before they can be defined.  A declaration
establishes that name as a terminal.  There may be multiple terminal
statements, each of which may declare multiple names.

   Terminals and Nonterminals share the same "namespace", meaning there
cannot be a terminal and a nonterminal having the same name.  By
convention, terminals identifiers are all caps and nonterminal
identifiers are capitalized, but it is up to the preference of the
grammar author...

     # format: terminal <NAME>;
     # format: terminal <NAME>, <NAME>, <NAME>;
     terminal IDENT;
     terminal T1, T2;

7.4 Terminal Definitions
========================

Terminal definitions are regular definitions; they associate a name with
an expression.  A regular expression is enclosed in double-quotes;
whitespace within the string is insignificant.  *Note Regular Expression
Syntax::, about how regular expressions are written in `STT'.

     # format: <TERMINAL> matches "regexp";
     IDENT matches " [_a-zA-Z0-9] [-_a-zA-Z0-9]* ";

7.5 Nonterminal Declarations
============================

Nonterminal declarations are identical to terminal declarations with the
exception of the keyword.  Nonterminal identifiers are by convention
capitalized.

     # format: nonterminal <NAME>;
     # format: nonterminal <NAME>, <NAME>, <NAME>;
     nonterminal Goal;
     nonterminal IdentList, Name, Statement;

7.6 Nonterminal Definitions
===========================

Nonterminal definitions are productions: each production relates a
nonterminal to a sequence of grammar symbols; when that sequence of
grammar symbols (terminals or nonterminal) appears the top of the parse
stack, the parser will reduce it to the nonterminal named in the
production (i.e. the nonterminal definition).

     # format: reduce <NONTERMINAL> when <SYMBOL> <SYMBOL> <SYMBOL>;
     reduce Term when Term PLUS Factor;

7.7 Accept Definition
=====================

This section consists of a single statement that states what "goal
symbol" must be reduced in order for the grammar to signal acceptance of
the input.  The goal symbol must be a declared nonterminal.  The
convention is "Goal".

     # format: accept when <NONTERMINAL>;
     accept when Goal;

7.8 Context Declarations
========================

The context declarations and definitions are optional.  *Note Lexical
Context::, for an explanation of what a "context" is.

   The context declarations section is similar to the terminal
declarations section and nonterminal declarations section.

     # format: context <NAME>;
     # format: context <NAME>, <NAME>, <NAME>;
     context comment;
     context special1, special2;

   Identifiers used for contexts have their own namespace, each one must
be unique only within the set of context declarations.  The context
names "default" and "all" have special meaning.

7.9 Context Definitions
=======================

A context definition determines what subset of terminals in the full set
of terminals is "included" in the context.  If a terminal is included
within a particular context, its corresponding DFA will recognize the
appropriate character sequence (given the opportunity).

   Each context definition statement consists of a context name and a
list of one or more "context stack instructions".  A stack instruction
can say one of three things: "when terminal `X' is matched, do nothing",
"when terminal `X' is matched, switch into context `Y'", and "when
terminal `X' is matched, return to the previous context".

   * The default instruction for all terminals in a context that do not
     explicitly name an instruction is PEEK, meaning "do not change
     context; do nothing".

          # Implicit PEEK instructions for R, S, and T in context "default".
          default includes R, S, T;

   * An `shifts' instruction changes the lexer context to the named
     context.

          # Implicit PEEK instruction for X; PUSH for Y in context "default".
          default includes X, Y shifts special;

   * An `unshifts' instruction changes the lexer context to the previous
     context.

          # POP instruction for Z in context "special".
          special includes Z unshifts;

   The following example demonstrates the use of context switching
through context stack instructions:

     # format: <NAME> includes <INSTRUCTION>;
     terminal WHITESPACE, START_COMMENT, COMMENT_DATA, END_COMMENT;

     context default, comment;

     default includes START_COMMENT shifts comment, WHITESPACE;
     comment includes COMMENT_DATA, END_COMMENT unshifts;

7.10 Start Context Definition
=============================

This section defines what the starting context will be.  When omitted,
the default context is "default".

     # format: start with context <NAME>;
     start with context special;

7.11 Context Post-Processing
============================

After the grammar is parsed, some processing is done to initialize each
context with the terminals that will be included in it.

Case 1: No explicit contexts
============================

The simplest case is when no context information has been explicitly
provided -- the grammar consists of terminal and nonterminal
declarations/definitions only.

   In this circumstance, the processor implicitly adds in a single
context "default", and all terminals are added to that context.  The
lexer acts on the corresponding DFA and no context switching is done.

     terminal WHITESPACE, DATA;

     nonterminal data;
     reduce data when DATA;
     accept when data

Case 2: One or more explicit contexts, no "all" context
=======================================================

In this circumstance the user declares more or more contexts.  The
"default" context is always implicitly declared, but can be declared
explicitly with no error.

     terminal WHITESPACE, DATA,
              START_QUOTE, QUOTE_DATA, END_QUOTE;

     nonterminal data, quote;

     context quoted_context;

     default        includes WHITESPACE, DATA, START_QUOTE shifts quoted_context;
     quoted_context includes WHITESPACE, QUOTE_DATA, END_QUOTE unshifts;

Case 3: Use of the "all" context
================================

The "all" context is special in that it does not actually refer to a
real context (a DFA), but rather is a syntactic convenience.  Terminals
included in the "all" context are placed into every other context after
the grammar is parsed.  The all context does not have to be declared.

     terminal WHITESPACE, DATA,
              START_QUOTE, QUOTE_DATA, END_QUOTE;

     nonterminal data, quote;

     context quoted_context;

     all            includes WHITESPACE;
     default        includes DATA, START_QUOTE shifts quoted_context;
     quoted_context includes QUOTE_DATA, END_QUOTE unshifts;

8 Regular Expression Syntax
***************************

The regular expression syntax in STT is pretty standard.  Whitespace is
never significant, however - any literal space characters must be
introduced with the space character escape `\s'.  Any literal
double-quotation mark `"' must be escaped since regexps are always
enclosed in double-quotations.

8.1 List Operators
==================

`"Op"'
     "Definition"

`|'
     Union: a list of alternate choices that can be matched, like
     `a|b|c'.

`"none"'
     Concatenation: a list of atoms that must be matched in sequence,
     like `a b c' or `abc'.

`[]'
     Character Classes: A syntactic convenience for alternation of
     character intervals: `[\r\n]', `[a-z]'.  Negation of character
     classes inverts the sense of the inclusion: `[^a-z]'.  If the dash
     character `-' is one of the characters in the class, it must be the
     first member in the class: `[-=+]'.  Whitespace within the brackets
     is not significant and characters that would normally have to be
     escaped do not.  The ones that do include: backslash `\\',
     close-bracket `\', semicolon `\;', and all the whitespace escapes
     `\s', `\r', `\n', `\t', `\v'.  Octal and Unicode escapes can be
     used as well.

8.2 Quantification Operators
============================

`"Op"'
     "Definition"

`*'
     Closure: zero-or-more occurrences must exist

`+'
     Positive-closure: one or more occurrences must exist

`?'
     Optional: zero-or-one occurrences must exist

8.3 Literal Escapes
===================

`"Op"'
     "Definition"

`\\'
     literal backslash

`\s'
     literal space

`\n'
     literal newline

`\r'
     literal carriage return

`\t'
     literal horizontal tab

`\v'
     literal vertical tab

`\+'
     literal plus sign

`\*'
     literal asterisk

`\?'
     literal question-mark

`\('
     literal open-parenthesis

`\)'
     literal close-parenthesis

`\['
     literal open-bracket

`\]'
     literal close-bracket

`\|'
     literal pipe

`\"'
     literal double-quote (necessary since regexps are enclosed in
     double quotes)

8.4 Octal and Unicode Escapes
=============================

Octal and unicode escapes match the following regular expressions,
respectively:

     OCTAL_ESCAPE   matches " \\ [0-3] [0-7] [0-7] ";
     UNICODE_ESCAPE matches " \\ u [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] ";

8.5 Precedence
==============

From lowest to highest: union, concatentation, quantification, atom
(char | escape | char-class), grouping.

8.6 Examples
============

     IDENTIFIER matches " [_a-z] [-_a-zA-Z0-9] ";
     WHITESPACE matches " [\n \r \t \v \s]+ ";
     BEVERAGE   matches " coffee | tea | cola ";
     CAFFEINE   matches " caff(ei|ie)ne ";

9 Properties
************

Properties are used to pass variables to the grammar processor; they do
not affect the language itself, but assist in the generation process.

9.0.1 General properties
------------------------

`verbose'
     A boolean value that, if "true", will write verbose messages to the
     log.

9.0.2 Run-time properties
-------------------------

`run-error-limit'
     A number that, if defined, places a ceiling on the number of errors
     that can occur during a translation before aborting.

`run-lexer-debug'
     A boolean value that, if "true", will signal the `Lexer' to output
     debugging messages to the log.

`run-parser-debug'
     A boolean value that, if "true", will signal the `Parser' to output
     debugging messages to the log.

`run-interpreter-debug'
     A boolean value that, if "true", will signal the
     `LRTranslatorInterpreter' to output debugging messages to the log.

`run-print-parse-tree'
     A boolean value that, if "true", will signal the
     `com.inxar.syntacs.Run' class to printout the parse tree.

9.0.3 Compile-time properties that should be in every grammar
-------------------------------------------------------------

`author'
     A string that names the author or authors of the grammar.

`author-email'
     A string that names the email address/addresses of the
     author/authors.

`copyright'
     A string that will be included in generated files.  Defaults to
     "Copyright $YEAR $AUTHOR, $AUTHOR_EMAIL".

`compile-namespace'
     A string that names the package membership for the generated
     classes.  Example: "com.inxar.syntacs.translator.test".

`compile-sourcepath'
     A string that names the filesystem directory where the generated
     classes should be written.  Example: "./src".

`compile-dpa-constructor-method'
     A string that names the strength of the algorithm used to generate
     the DPA (the parse tables).  Can be one of `lalr1', `lr1', `slr1'.
     The default is lalr1, so this property is NOT NECESSARY for those
     grammars that choose LALR1.

`compile-interpreter-classname'
     A string that names the class to be used by the generated
     translator.  Example:
     "com.inxar.syntacs.translator.regexp.RegexpInterpreter".  This is
     the thing you have to implement for your grammar.  The default
     value is "com.inxar.syntacs.translator.lr.StandardLRInterpreter",
     which builds a concrete syntax tree and is useful for testing.

9.0.4 Compile-time properties that affect class generation (development use)
----------------------------------------------------------------------------

`compile'
     A boolean value that, if "false", will prevent any implementation
     classes from being generated and written to the filesystem.

`compile-lexical'
     A boolean value that, if "false", will prevent any lexical analysis
     classes from being generated and written to the filesystem.

`compile-syntactic'
     A boolean value that, if "false", will prevent the syntax analysis
     classes from being generated and written to the filesystem.

`compile-grammar'
     A boolean value that, if "false", will prevent the
     `LRTranslatorGrammar' class from being generated and written to the
     filesystem.

`compile-pickle'
     A boolean value that, if "false", will prevent classes from using
     an int array packing technique called "pickling" (decreases
     bytecode size).  The code seems stable, so this should not be
     needed.

`compile-pickle-dfa'
     A boolean value that, if "false", will prevent pickling of
     generated DFA classes.

`compile-pickle-dpa'
     A boolean value that, if "false", will prevent pickling of
     generated DPA classes.

9.0.5 Compile-time properties that customize the translation components
-----------------------------------------------------------------------

`compile-input-classname'
     A string that names the class which implements `Input' that will be
     used by the generated translator at run-time.

`compile-lexer-classname'
     A string that names the class which implements `Lexer' that will be
     used by the generated translator at run-time.

`compile-parser-classname'
     A string that names the class which implements `Parser' that will
     be used by the generated translator at run-time.

`compile-interpreter-classname'
     A string that names the class which implements
     `LRTranslatorInterpreter' that will be used by the generated
     translator at run-time.

`compile-dpa-constructor-classname'
     A string that names the class which implements `DPAConstructor'
     that will be used by the grammar compiler at compile-time to
     construct a DPA.

9.0.6 Compile-time debugging properties
---------------------------------------

`compile-grammar-regular-debug'
     A boolean value that, if "true", will printout the regular
     grammars.

`compile-grammar-context-free-debug'
     A boolean value that, if "true", will printout the context-free
     grammar.

`compile-dfa-debug'
     A boolean value that, if "true", will printout the grammar DFAs.

`compile-dpa-debug'
     A boolean value that, if "true", will printout the grammar DPA.

9.0.7 Compile-time properties that control GraphViz output
----------------------------------------------------------

`viz'
     A boolean value that, if "false", will prevent any generation of
     GraphViz dot files.

`viz-lexical'
     A boolean value that, if "true", will generate graphviz .dot files
     for DFA instances.

`viz-syntactic'
     A boolean value that, if "true", will generate graphviz .dot files
     for DPA instances.

`viz-namespace'
     A string that names the package membership for the generated dot
     files.  Defaults to value of `compile-namespace'.

`viz-sourcepath'
     A string that names the filesystem directory where the generated
     dot files should be written.  Defaults to value of
     `compile-sourcepath'.

`viz-dfa-size'
     A string that names the size of the postscript bounding box for the
     DPA graph (see graphviz manual).

`viz-dfa-rankdir'
     A string that names the rank direction of the DFA graph (see
     graphviz manual).

`viz-dfa-concentrate-edges'
     A boolean that, if "true", will heuristically merge edges when
     appropriate (see graphviz manual).

`viz-dfa-node-color'
     A string that controls the color of the DFA nodes (see graphviz
     manual).

`viz-dfa-node-shape'
     A string that controls the shape of the DFA nodes (see graphviz
     manual).

`viz-dfa-edge-color'
     A string that controls the color of the DFA edges (see graphviz
     manual).

`viz-dfa-edge-style'
     A string that controls the style of the DFA edges (see graphviz
     manual).

`viz-dfa-label-edge-color'
     A string that controls the color of the DFA label edges (see
     graphviz manual).

`viz-dfa-label-edge-style'
     A string that controls the style of the DFA label edges (see
     graphviz manual).

`viz-dpa-size'
     A string that names the size of the postscript bounding box for the
     DPA graph (see graphviz manual).

`viz-dpa-rankdir'
     A string that names the rank direction of the DPA graph (see
     graphviz manual).

`viz-dpa-concentrate-edges'
     A boolean that, if "true", will heuristically merge edges when
     appropriate (see graphviz manual).

`viz-dpa-node-color'
     A string that controls the color of the DPA nodes (see graphviz
     manual).

`viz-dpa-node-shape'
     A string that controls the shape of the DPA nodes (see graphviz
     manual).

`viz-dpa-hide-terminal-edges'
     A boolean that, if "true", will prevent visualization of terminal
     transitions.

`viz-dpa-terminal-edge-color'
     A string that controls the color of the DPA terminal edges (see
     graphviz manual).

`viz-dpa-terminal-edge-style'
     A string that controls the style of the DPA terminal edges (see
     graphviz manual).

`viz-dpa-hide-nonterminal-edges'
     A boolean that, if "true", will prevent visualization of
     nonterminal transitions.

`viz-dpa-terminal-edge-color'
     A string that controls the color of the DPA nonterminal edges (see
     graphviz manual).

`viz-dpa-terminal-edge-style'
     A string that controls the style of the DPA nonterminal edges (see
     graphviz manual).

`viz-dpa-hide-loopback-edges'
     A boolean that, if "true", will prevent visualization of loopback
     edges (closely related to reductions).

`viz-dpa-loopback-edge-color'
     A string that controls the color of the DPA loopback edges (see
     graphviz manual).

`viz-dpa-loopback-edge-style'
     A string that controls the style of the DPA loopback edges (see
     graphviz manual).

   A perusal of the source code might uncover undocumented properties.

10 Example Grammar
******************

The regular expression grammar will be used as an example.

10.1 The Grammar
================


     # GRAMMAR DECLARATION
     this is regexp version 0.1.0;

     # PROPERTY DEFINITIONS
     property compile-sourcepath = "./src";
     property compile-namespace = "com.inxar.syntacs.translator.regexp";
     property compile-interpreter-classname =
         "com.inxar.syntacs.translator.regexp.RegexpInterpreter";

     # TERMINAL DECLARATIONS
     terminal WHITESPACE;
     terminal CHAR;
     terminal CHAR_CLASS_CHAR;
     terminal PIPE;
     terminal STAR;
     terminal QUESTION;
     terminal PLUS;
     terminal OPEN_PAREN;
     terminal CLOSE_PAREN;
     terminal OPEN_BRACKET;
     terminal OPEN_BRACKET_CARET;
     terminal OPEN_BRACKET_DASH;
     terminal OPEN_BRACKET_CARET_DASH;
     terminal CLOSE_BRACKET;
     terminal CHAR_CLASS_DASH;
     terminal ESC_PIPE;
     terminal ESC_STAR;
     terminal ESC_QUESTION;
     terminal ESC_PLUS;
     terminal ESC_OPEN_PAREN;
     terminal ESC_CLOSE_PAREN;
     terminal ESC_OPEN_BRACKET;
     terminal ESC_CLOSE_BRACKET;
     terminal ESC_BACKSLASH;
     terminal ESC_SPACE;
     terminal ESC_TAB;
     terminal ESC_VERTICAL_TAB;
     terminal ESC_CR;
     terminal ESC_LF;
     terminal ESC_OCTAL;
     terminal ESC_UNICODE;

     # TERMINAL DEFINITIONS
     WHITESPACE matches "(\t|\n|\v|\r|\s)+";
     CHAR matches "[^\\|()[\]*+?]";
     CHAR_CLASS_CHAR matches "[^-\]\\]";
     PIPE matches "\|";
     STAR matches "\*";
     QUESTION matches "\?";
     PLUS matches "\+";
     OPEN_PAREN matches "\(";
     CLOSE_PAREN matches "\)";
     OPEN_BRACKET matches "(\[(\t|\n|\v|\r|\s)*)";
     OPEN_BRACKET_CARET matches "(\[(\t|\n|\v|\r|\s)*^)";
     OPEN_BRACKET_DASH matches "(\[(\t|\n|\v|\r|\s)*-)";
     OPEN_BRACKET_CARET_DASH matches "(\[(\t|\n|\v|\r|\s)*^(\t|\n|\v|\r|\s)*-)";
     CLOSE_BRACKET matches "\]";
     CHAR_CLASS_DASH matches "(-)";
     ESC_PIPE matches "(\\\|)";
     ESC_STAR matches "(\\\*)";
     ESC_QUESTION matches "(\\\?)";
     ESC_PLUS matches "(\\\+)";
     ESC_OPEN_PAREN matches "(\\\()";
     ESC_CLOSE_PAREN matches "(\\\))";
     ESC_OPEN_BRACKET matches "(\\\[)";
     ESC_CLOSE_BRACKET matches "(\\\])";
     ESC_BACKSLASH matches "(\\\\)";
     ESC_SPACE matches "(\\s)";
     ESC_TAB matches "(\\t)";
     ESC_VERTICAL_TAB matches "(\\v)";
     ESC_CR matches "(\\r)";
     ESC_LF matches "(\\n)";
     ESC_OCTAL matches "(\\0[0-3][0-7][0-7])";
     ESC_UNICODE matches "(\\u[0-9a-fA-F][0-9a-fA-F][0-9a-fA-F][0-9a-fA-F])";

     # NONTERMINAL DECLARATIONS
     nonterminal Goal;
     nonterminal Union;
     nonterminal Concat;
     nonterminal Term;
     nonterminal Quantifier;
     nonterminal Atom;
     nonterminal CharClass;
     nonterminal CharClassBegin;
     nonterminal CharClassTermList;
     nonterminal CharClassTerm;
     nonterminal CharClassAtom;

     # NONTERMINAL DEFINITIONS
     reduce Goal when Union;
     reduce Union when Concat;
     reduce Union when Union PIPE Concat;
     reduce Concat when Term;
     reduce Concat when Concat Term;
     reduce Term when Atom;
     reduce Term when Atom Quantifier;
     reduce Quantifier when STAR;
     reduce Quantifier when PLUS;
     reduce Quantifier when QUESTION;
     reduce Atom when CHAR;
     reduce Atom when ESC_BACKSLASH;
     reduce Atom when ESC_PIPE;
     reduce Atom when ESC_PLUS;
     reduce Atom when ESC_STAR;
     reduce Atom when ESC_QUESTION;
     reduce Atom when ESC_OPEN_BRACKET;
     reduce Atom when ESC_CLOSE_BRACKET;
     reduce Atom when ESC_OPEN_PAREN;
     reduce Atom when ESC_CLOSE_PAREN;
     reduce Atom when ESC_SPACE;
     reduce Atom when ESC_TAB;
     reduce Atom when ESC_VERTICAL_TAB;
     reduce Atom when ESC_CR;
     reduce Atom when ESC_LF;
     reduce Atom when ESC_OCTAL;
     reduce Atom when ESC_UNICODE;
     reduce Atom when CharClass;
     reduce Atom when OPEN_PAREN Union CLOSE_PAREN;
     reduce CharClass when CharClassBegin CharClassTermList CLOSE_BRACKET;
     reduce CharClass when OPEN_BRACKET_CARET_DASH CLOSE_BRACKET;
     reduce CharClassBegin when OPEN_BRACKET;
     reduce CharClassBegin when OPEN_BRACKET_CARET;
     reduce CharClassBegin when OPEN_BRACKET_DASH;
     reduce CharClassBegin when OPEN_BRACKET_CARET_DASH;
     reduce CharClassTermList when CharClassTerm;
     reduce CharClassTermList when CharClassTermList CharClassTerm;
     reduce CharClassTerm when CharClassAtom;
     reduce CharClassTerm when CharClassAtom CHAR_CLASS_DASH CharClassAtom;
     reduce CharClassAtom when CHAR_CLASS_CHAR;
     reduce CharClassAtom when ESC_BACKSLASH;
     reduce CharClassAtom when ESC_CLOSE_BRACKET;
     reduce CharClassAtom when ESC_SPACE;
     reduce CharClassAtom when ESC_TAB;
     reduce CharClassAtom when ESC_VERTICAL_TAB;
     reduce CharClassAtom when ESC_CR;
     reduce CharClassAtom when ESC_LF;
     reduce CharClassAtom when ESC_OCTAL;
     reduce CharClassAtom when ESC_UNICODE;

     accept when Goal;

     # CONTEXT DECLARATIONS
     context default;
     context charclass;

     # CONTEXT DEFINITIONS
     default includes WHITESPACE, CHAR, PIPE, STAR, QUESTION, PLUS, OPEN_PAREN, CLOSE_PAREN,
                      ESC_PIPE, ESC_QUESTION, ESC_STAR, ESC_PLUS, ESC_OPEN_PAREN, ESC_CLOSE_PAREN,
                      ESC_OPEN_BRACKET, ESC_BACKSLASH, ESC_SPACE, ESC_TAB, ESC_VERTICAL_TAB,
                      ESC_CR, ESC_LF, ESC_OCTAL, ESC_UNICODE, ESC_CLOSE_BRACKET,
                      OPEN_BRACKET shifts charclass,
                      OPEN_BRACKET_CARET shifts charclass,
                      OPEN_BRACKET_DASH shifts charclass,
                      OPEN_BRACKET_CARET_DASH shifts charclass;

     charclass includes WHITESPACE, CHAR_CLASS_CHAR, CHAR_CLASS_DASH,
                        ESC_BACKSLASH, ESC_SPACE, ESC_TAB, ESC_VERTICAL_TAB, ESC_CR, ESC_LF,
                        ESC_OCTAL, ESC_UNICODE, ESC_CLOSE_BRACKET,
                        CLOSE_BRACKET unshifts;

     start in context default;

   Several things to note in the grammar:

   * There are multiple contexts; character classes have their own set
     of regular definitions that are not shared across all contexts.

   * Whitespace and a number of ESC_xxx tokens are members of both
     contexts.

   * `CLOSE_BRACKET' returns the lexer the to the previous lexical
     context.

10.2 The TranslatorGrammar
==========================

Notice that the grammar is "code-free"; it does not contain any Java
code.  This means that user-defined code has to go somewhere else.  The
paradigm used by the STT is to sandwich an object between the lexer and
the parser that is reponsible for interpreting lexer match events.  A
similar interface exists between the parser and the parser-interpreter
with reduction events.

   Terminal match events are signaled to the `LexerInterpreter' through
the `match(int terminal_type, int offset, int length' method.  The first
argument identifies what kind of terminal (i.e. token) was matched,
represented as a number.  Therefore, each terminal is given a unique
number that identifies it at the time of declaration.

     Note: since terminal declarations are separate from terminal
     definitions (the regular expression), this means that some
     terminals may never be signaled by the lexer.  The rationale behind
     this is to allow the user more flexibility in how they want to
     structure their grammar: some authors might prefer to match all
     keywords using a single regular expression, do a symbol lookup,
     then pass along the appropriate terminal constant to the parser.
     This has the potential to have much smaller lexer transition
     tables.

     Therefore, the set of terminals known to the lexer and the set of
     terminals known to the parser may be non-equal.

   Unique numbers are assigned to each terminal, nonterminal, and
production in the grammar.  When the grammar is read by the syntacs
processor, the first thing it generates is the `TranslatorGrammar'.
This class contains all the context, terminal, nonterminal, and
production constants as well as functions like `getTerminal(int ID)'
that returns the name of a terminal by number.

   These constants are used in the switch statements of the
`LexerInterpreter' and `ParserInterpreter' implementations.

   The `TranslatorGrammar' object is also a factory for `Translator'
instances of that grammar.  Therefore, to get an instance of a
`Translator' that parses regular expressions:

     TranslatorGrammar tg = new com.inxar.syntacs.translator.regexp.RegexpGrammar();
     Translator t = tg.newTranslator();

   To generate the translator grammar, run the syntacs compiler on the
grammar:

     [user@host]$ sttc regexp.stt

10.3 The LexerInterpreter
=========================

Once you have the `TranslatorGrammar' and the grammar constants, you can
write the code for the `LexerInterpreter' and `ParserInterpreter'.

   For the `LexerInterpreter', the key method to implement is `match(int
terminal_type, int offset, int length'.  Within this method the
`LexerInterpreter' presumably does something significant with the output
of the lexer.

   For our purposes, the `LexerInterpreter' will generate new `Symbol'
instances and pass them to the parser.  The `Symbol' interface defines a
method `getSymbolType()' that identifies it as a member of the
context-free grammar which the parser was built to recognize.

   Here is ther salient code:

    public void match(int type, int off, int len)
    {
	Symbol symbol = null;

        switch (type) {

	case RegexpGrammar.T_WHITESPACE:
	    return;

	case RegexpGrammar.T_PIPE:
	case RegexpGrammar.T_OPEN_PAREN:
	case RegexpGrammar.T_CLOSE_PAREN:
	case RegexpGrammar.T_CHAR_CLASS_DASH:
	case RegexpGrammar.T_CLOSE_BRACKET:
	case Token.STOP:
	    symbol = new ConstantSymbol(type);
	    break;

	case RegexpGrammar.T_CHAR:
	case RegexpGrammar.T_CHAR_CLASS_CHAR:
	    if (len != 1)
		throw new InternalError("Expected token to be only one character.");
	    symbol = newAtom( type, in.retch(off) );
	    break;

	case RegexpGrammar.T_ESC_OCTAL:
	    {
		char c = (char)0;
		try {
		    c = (char)Integer.parseInt( in.stretch(off + 1, len - 1), 8 );
		} catch (NumberFormatException nfex) {
		    trace("NumberFormatException caught while trying to parse "+
			  in.stretch(off + 1, len - 1));
		    nfex.printStackTrace();
		}
		symbol = newAtom(type, c);
		break;
	    }

	case RegexpGrammar.T_ESC_UNICODE:
	    {
		char c = (char)0;
		try {
		    c = (char)Integer.parseInt( in.stretch(off + 2, len - 2), 16 );
		} catch (NumberFormatException nfex) {
		    nfex.printStackTrace();
		}
		symbol = newAtom(type, c);
		break;
	    }

	case RegexpGrammar.T_ESC_SPACE:         symbol = newAtom(type, ' '); break;
	case RegexpGrammar.T_ESC_TAB:           symbol = newAtom(type, '\t'); break;
	case RegexpGrammar.T_ESC_VERTICAL_TAB:  symbol = newAtom(type, '\013'); break;
	case RegexpGrammar.T_ESC_CR:            symbol = newAtom(type, '\r'); break;
	case RegexpGrammar.T_ESC_LF:            symbol = newAtom(type, '\n'); break;
	case RegexpGrammar.T_ESC_BACKSLASH:     symbol = newAtom(type, '\\'); break;
	case RegexpGrammar.T_ESC_CLOSE_BRACKET: symbol = newAtom(type, ']'); break;
	case RegexpGrammar.T_ESC_PIPE:          symbol = newAtom(type, '|'); break;
	case RegexpGrammar.T_ESC_QUESTION:      symbol = newAtom(type, '?'); break;
	case RegexpGrammar.T_ESC_STAR:          symbol = newAtom(type, '*'); break;
	case RegexpGrammar.T_ESC_PLUS:          symbol = newAtom(type, '+'); break;
	case RegexpGrammar.T_ESC_OPEN_BRACKET:  symbol = newAtom(type, '['); break;
	case RegexpGrammar.T_ESC_OPEN_PAREN:    symbol = newAtom(type, '('); break;
	case RegexpGrammar.T_ESC_CLOSE_PAREN:   symbol = newAtom(type, ')'); break;

	case RegexpGrammar.T_OPEN_BRACKET:
	    symbol = newClass(type, false, false); break;
	case RegexpGrammar.T_OPEN_BRACKET_CARET:
	    symbol = newClass(type, true, false); break;
	case RegexpGrammar.T_OPEN_BRACKET_DASH:
	    symbol = newClass(type, false, true); break;
	case RegexpGrammar.T_OPEN_BRACKET_CARET_DASH:
	    symbol = newClass(type, true, true); break;

	case RegexpGrammar.T_STAR:
	    symbol = new QuantifierSymbol(type, Regexp.CLOSURE); break;
	case RegexpGrammar.T_QUESTION:
	    symbol = new QuantifierSymbol(type, Regexp.OPTIONAL); break;
	case RegexpGrammar.T_PLUS:
	    symbol = new QuantifierSymbol(type, Regexp.PCLOSURE); break;

	default:
	    throw new InternalError
		("Expected terminal type: " + grammar.getTerminal(type));
        }

	parser.notify(symbol);
    }

    private Symbol newAtom(int type, char value)
    {
	RegexpAtom atom = new RegexpAtom();
	atom.setValue( value );
	atom.setSymbolType(type);
	return atom;
    }

    private Symbol newClass(int type, boolean isNegated, boolean hasDash)
    {
	CharClassBeginSymbol ccbs = new CharClassBeginSymbol(type);
	ccbs.isNegated = isNegated;
	ccbs.hasDash = hasDash;
	return ccbs;
    }

   Some points from the code:

   * WHITESPACE is ignored by returning immediately; no symbol is passed
     to the parser.

   * Several different `Symbol' implementations are used:
     `ConstantSymbol' requires no interaction with the `Input', other
     types need little to no interaction with the `Input'.  This
     minimizes unnecessary arraycopying in the tightest part of the
     parse loop.

   * Multiple different cases can be aggregated to the same switch
     block.

   * The last statement passes the symbol along to the parser.

10.4 The ParserInterpreter
==========================

The `ParserInterpreter' is implemented in a similar manner: the reduce
method is generally a big switch block that handles each production.
The `ParserInterpreter' can do whatever it wants with the symbols that
are currently on the top of the parse stack -- if they are to be
retained in the parse tree, the `ParserInterpreter' must fetch it from
the stack and include it into the nonterminal symbol that will be
returned to the parser.

   The `Sentence' object abstracts the part of the parse stack that is
being reduced such that indices into the stack refer to the expected
component of the production.  For example, when the production
`P_CharClass__CharClassBegin_CharClassTermList_CLOSE_BRACKET' is being
reduced, `sentence.at(0)' accesses the `CharClassBegin' symbol,
`sentence.at(1)' accesses `CharClassTermList', and `sentence.at(2)'
accesses `CLOSE_BRACKET'.

    public Symbol reduce(int type, Sentence s)
    {
	Symbol symbol = null;

	switch (type) {

	case RegexpGrammar.P_Quantifier__STAR:
	case RegexpGrammar.P_Quantifier__PLUS:
	case RegexpGrammar.P_Quantifier__QUESTION:
	case RegexpGrammar.P_Term__Atom:
	case RegexpGrammar.P_Atom__CHAR:
	case RegexpGrammar.P_Atom__ESC_BACKSLASH:
	case RegexpGrammar.P_Atom__ESC_PIPE:
	case RegexpGrammar.P_Atom__ESC_PLUS:
	case RegexpGrammar.P_Atom__ESC_STAR:
	case RegexpGrammar.P_Atom__ESC_QUESTION:
	case RegexpGrammar.P_Atom__ESC_OPEN_BRACKET:
	case RegexpGrammar.P_Atom__ESC_CLOSE_BRACKET:
	case RegexpGrammar.P_Atom__ESC_OPEN_PAREN:
	case RegexpGrammar.P_Atom__ESC_CLOSE_PAREN:
	case RegexpGrammar.P_Atom__ESC_SPACE:
	case RegexpGrammar.P_Atom__ESC_TAB:
	case RegexpGrammar.P_Atom__ESC_VERTICAL_TAB:
	case RegexpGrammar.P_Atom__ESC_CR:
	case RegexpGrammar.P_Atom__ESC_LF:
	case RegexpGrammar.P_Atom__ESC_OCTAL:
	case RegexpGrammar.P_Atom__ESC_UNICODE:
	case RegexpGrammar.P_Atom__CharClass:
	case RegexpGrammar.P_CharClassTerm__CharClassAtom:
	case RegexpGrammar.P_CharClassAtom__CHAR_CLASS_CHAR:
	case RegexpGrammar.P_CharClassAtom__ESC_BACKSLASH:
	case RegexpGrammar.P_CharClassAtom__ESC_CLOSE_BRACKET:
	case RegexpGrammar.P_CharClassAtom__ESC_SPACE:
	case RegexpGrammar.P_CharClassAtom__ESC_TAB:
	case RegexpGrammar.P_CharClassAtom__ESC_VERTICAL_TAB:
	case RegexpGrammar.P_CharClassAtom__ESC_CR:
	case RegexpGrammar.P_CharClassAtom__ESC_LF:
	case RegexpGrammar.P_CharClassAtom__ESC_OCTAL:
	case RegexpGrammar.P_CharClassAtom__ESC_UNICODE:
	case RegexpGrammar.P_CharClassBegin__OPEN_BRACKET:
	case RegexpGrammar.P_CharClassBegin__OPEN_BRACKET_CARET:
	case RegexpGrammar.P_CharClassBegin__OPEN_BRACKET_DASH:
	case RegexpGrammar.P_CharClassBegin__OPEN_BRACKET_CARET_DASH:
	    {
		symbol =  s.at(0);
		break;
	    }

	case RegexpGrammar.P_Union__Concat:
	    {
		RegexpList union = new RegexpList(Regexp.UNION);
		union.addRegexp( (Regexp)s.at(0) );
		symbol = union;
		break;
	    }

	case RegexpGrammar.P_Concat__Term:
	    {
		RegexpList concat = new RegexpList(Regexp.CONCAT);
		concat.addRegexp( (Regexp)s.at(0) );
		symbol = concat;
		break;
	    }

	case RegexpGrammar.P_Concat__Concat_Term:
	    {
		RegexpList list = (RegexpList)s.at(0);
		list.addRegexp( (Regexp)s.at(1) );
		symbol = list;
		break;
	    }

	case RegexpGrammar.P_Union__Union_PIPE_Concat:
	    {
		RegexpList list = (RegexpList)s.at(0);
		list.addRegexp( (Regexp)s.at(2) );
		symbol = list;
		break;
	    }

	case RegexpGrammar.P_CharClassTermList__CharClassTerm:
	    {
		symbol = new ListSymbol(type, s.at(0));
		break;
	    }

	case RegexpGrammar.P_CharClassTermList__CharClassTermList_CharClassTerm:
	    {
		ListSymbol sym = (ListSymbol)s.at(0);
		sym.list.add(s.at(1));
		symbol = sym;
		break;
	    }

	case RegexpGrammar.P_Term__Atom_Quantifier:
	    {
		QuantifierSymbol q = (QuantifierSymbol)s.at(1);
		RegexpTerm term = new RegexpTerm(q.regexpType);
		term.setInternal( (Regexp)s.at(0) );
		symbol = term;
		break;
	    }

	case RegexpGrammar.P_Atom__OPEN_PAREN_Union_CLOSE_PAREN:
	    {
		RegexpTerm term = new RegexpTerm(Regexp.GROUP);
		term.setInternal( (Regexp)s.at(1) );
		symbol = term;
		break;
	    }

	case RegexpGrammar.P_CharClassTerm__CharClassAtom_CHAR_CLASS_DASH_CharClassAtom:
	    {
		RegexpAtom lo = (RegexpAtom)s.at(0);
		RegexpAtom hi = (RegexpAtom)s.at(2);
		symbol = new RegexpRange(lo, hi);
		break;
	    }

	case RegexpGrammar.P_CharClass__CharClassBegin_CharClassTermList_CLOSE_BRACKET:
	    {
		RegexpCharClass cc = new RegexpCharClass();
		CharClassBeginSymbol ccbs = (CharClassBeginSymbol)s.at(0);
		cc.isNegated(ccbs.isNegated);
		cc.hasDash(ccbs.hasDash);
		List list = ((ListSymbol)s.at(1)).list;
		cc.setList(list);
		symbol = cc;
		break;
	    }

	case RegexpGrammar.P_CharClass__OPEN_BRACKET_CARET_DASH_CLOSE_BRACKET:
	    {
		RegexpCharClass cc = new RegexpCharClass();
		CharClassBeginSymbol ccbs = (CharClassBeginSymbol)s.at(0);
		cc.isNegated(ccbs.isNegated);
		cc.hasDash(false);

		RegexpAtom dash_atom = new RegexpAtom();
		dash_atom.setValue('-');
		List list = new ArrayList();
		list.add(dash_atom);

		cc.setList(list);
		symbol = cc;
		break;
	    }

	case RegexpGrammar.P_Goal__Union:
	    {
		Regexp regexp = (Regexp)s.at(0);
		this.regexp = regexp;
		symbol = regexp;
		break;
	    }

	default:
      	    throw new InternalError("Unknown Production: "+grammar.getProduction(type));
	}

	return symbol;
    }

   * Each case picks out the relevant `Symbol' objects out of the parse
     stack through the `Sentence' interface (the top of the parse
     stack).

   * Multiple cases aggregate to the same code; they are handled the
     same.

   * 
   Once again, the `LexerInterpreter' and `ParserInterpreter' is
typically the same object that implements `LRTranslatorInterpreter'.
Once this object has been defined, you can set the property:

     property compile-interpreter-classname =
      "com.inxar.syntacs.translator.regexp.RegexpInterpreter";

   And regenerate the translator.  At this point the translator can be
used:

TranslatorGrammar tg = new com.inxar.syntacs.translator.regexp.RegexpGrammar();
Translator t = tg.newTranslator();

StringReader in = new StringReader("a|b|c");

try {
    Regexp regexp = (Regexp)translator.translate(in);
} catch (TranslationException ex) {
    ex.printStackTrace();
}

11 Graphviz
***********

Certain pieces of the STT such as the finite automata and pushdown
automata are capable of writing themselves as Graphviz
(http://www.research.att.com/sw/tools/graphviz/) "dot" files.  These
text files can then be converted to postscript using the tools in the
graphviz distribution and further converted to various image formats
using freely available postscript/image conversion tools.  My machine
has Ghostscript (http://www.cs.wisc.edu/~ghost/) and ImageMagick
(http://www.imagemagick.org/) installed and I have been very pleased
with them.

   To generate graphviz files, you need to add a few properties to the
grammar and recompile it.  The `.dot' files will be written where the
generated classes are (can override this with the `viz-namespace' and
`vis-sourcepath' properties).  To tell the processor to write the files,
say:

     property viz-lexical = "true";
     property viz-syntactic = "true";

   There are several properties which customize the dimensions, colors,
and styles of the generated files.  Note that visualization of all but
the most trivial of grammars is difficult at best since the complexity
of the graphs and the size of the images become prohibitive.  You'll
find yourself wanting to visualize those graphs anyway since it's
awfully cool, but you'll only get a gestalt view.  Try fiddling with the
graph size with the `vis-dfa-size' and `viz-dpa-size' properties (see
graphviz manual).

   *Note*: The license for GraphViz is "open-source", but it's not free
(http://www.gnu.org/philosophy/free-sw.html) - caveat emptor, and for
that matter, caveat vendor.

12 Resources
************

12.1 Books
==========

Compilers - Principles Techniques and Tools (http://images.amazon.com/images/P/0201100886.01.LZZZZZZZ.gif)
     The classic compiler text by Aho
     (http://www.columbia.edu/cu/record/archives/vol21/vol21_iss5/record2105.14c.gif),
     Sethi (http://cm.bell-labs.com/who/ravi/), and Ullman
     (http://www-db.stanford.edu/~ullman/) commonly known as the "Red
     Dragon Book", or just "Dragon Book".  Highly recommended for a
     substantive introduction to automata and parsing theory.

Languages, and Computation
     Despite its self-declaration of triviality in the title, this one
     by Hopcroft
     (http://www.cs.cornell.edu/annual_report/1997/hopcroft.htm) and
     Ullman is less introductory than others, if you ask me.  Still,
     it's a great book.  The link above is to the second edition -- I
     have the first edition that has a completely different graphic on
     the cover.  I can see why they changed it, but I kind-of wished
     they hadn't.

Mastering Regular Expressions (http://www.oreilly.com/catalog/regex/)
     While not a theoretical book, it is useful for practical learning
     about how to write a regular expression.

The Theory and Practice of Compiler Writing (http://www.cs.ualberta.ca/research/library/0-07-065161-2.html)
     This book by Jean-Paul Tremblay
     (http://www.cs.usask.ca/faculty/tremblay/) and Paul Sorenson
     (http://www.cs.ualberta.ca/people/faculty/sorenson.html) has a good
     alternate description of the DeRemer-Pennello LALR1 construction
     algorithm.  Out of print, but you can probably find it at your
     local univerity computer science library.  Please tell me he didn't
     actually shoot that beautiful animal.

Lex & Yacc (http://www.oreilly.com/catalog/lex/)
     A good overview of the classic lexer and parser compiler duo by
     Levine, Mason, and Brown.

12.2 Articles
=============

Efficient Computation of LALR(1) look-ahead sets (http://www.acm.org/pubs/citations/journals/toplas/1982-4-4/p615-deremer/)
     F. L. DeRemer and T. Pennello, ACM Transactions on Programming
     Languages and Systems, October 1982.  The canonical reference to
     the LALR1 construction algorithm used by this parser generator.
     Note their analysis of the relationship of SLR(1) and NQLALR(1)
     described in this paper is flawed (see "On the (non-)Relationship
     between SLR(1) and NQLALR(1) Grammars." M. E. Bermudez and K. M.
     Schimpf, ACM Transactions on Programming Languages and Systems,
     April 1988).

